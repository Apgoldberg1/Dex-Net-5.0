{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib\n",
    "data_path = \"gogs_test_data_real/raft_stereo1/depth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zed_depth_folder(pattern):\n",
    "    files = glob.glob(pattern)\n",
    "    depth_ims = np.zeros((len(files), 720, 1280, 1))\n",
    "    file_names = []\n",
    "    for i, file in enumerate(files):\n",
    "        float_values = np.loadtxt(file)\n",
    "        file_names.append(file)\n",
    "\n",
    "        np_arr = np.array(float_values).reshape(720, 1280, 1)\n",
    "        depth_ims[i] = np_arr\n",
    "    return depth_ims, file_names\n",
    "\n",
    "pattern = os.path.join(data_path, \"*.txt\")\n",
    "depth_ims, file_names = load_zed_depth_folder(pattern)\n",
    "print(np.isinf(depth_ims).sum())\n",
    "print(np.isnan(depth_ims).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(depth_ims.shape)\n",
    "print(np.isnan(depth_ims).sum())\n",
    "print(np.isinf(depth_ims).sum())\n",
    "\n",
    "def plot_arr(depth_ims, file_names=None, max_imgs=None):\n",
    "    cols = 4\n",
    "    max_imgs = min(max_imgs, len(depth_ims)) if max_imgs is not None else len(depth_ims)\n",
    "    # heights = {'height_ratios': [780]*((max_imgs // cols + 1)*cols)}\n",
    "    fig, axes = plt.subplots(max_imgs // cols + 1, cols, figsize=(12, 16))\n",
    "    for i, img in enumerate(depth_ims[:max_imgs]):\n",
    "        axes[i//cols, i%cols].imshow(img)\n",
    "        if file_names is not None:\n",
    "            axes[i//4, i%4].set_title(f'File: {Path(file_names[i]).stem}')\n",
    "        \n",
    "    for ax in axes.flatten():\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_arr(depth_ims, file_names, max_imgs=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_nan_clean(input_img):\n",
    "    # new_size = (512, 288)\n",
    "    # new_size = (720, 1280)\n",
    "    # image = cv2.resize(input_img.copy(), new_size)\n",
    "    # image[np.isnan(image)] = np.nanmin(image)\n",
    "    # image = image.astype(np.float32)\n",
    "\n",
    "    # mask = np.isnan(cv2.resize(input_img, new_size)).astype(np.uint8)\n",
    "\n",
    "    # cleaned_array = cv2.inpaint(image, mask, 5, cv2.INPAINT_TELEA)\n",
    "    # cleaned_array = cleaned_array.clip(0, 1000)\n",
    "\n",
    "    # assert np.isnan(cleaned_array).sum() == 0, \"still NaNs in image\"\n",
    "    cleaned_array = input_img.copy()\n",
    "    # cleaned_array[np.isnan(cleaned_array)] = np.percentile(input_img[~np.isnan(cleaned_array)], 10)\n",
    "\n",
    "    ### HARDCODED TABLE HEIGHT\n",
    "    cleaned_array[np.isnan(cleaned_array) | (cleaned_array > 1)] = .4\n",
    "\n",
    "    return cleaned_array\n",
    "\n",
    "cleaned_depth_ims = np.array([img_nan_clean(depth_im) for depth_im in depth_ims])\n",
    "# plot_arr(cleaned_depth_ims[:,:,100:750], file_names = file_names, max_imgs = 28)\n",
    "plot_arr(cleaned_depth_ims, file_names = file_names, max_imgs = 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dexnet.grasp_model import HighResFCGQCNN as FCGQCNN\n",
    "w = torch.load(\"/home/apgoldberg/Dex-Net-5.0/model_zoo/max_normal_normal_conversion.pth\")\n",
    "# from dexnet.grasp_model import DexNet3FCGQCNN as FCGQCNN\n",
    "# w = torch.load(\"/home/apgoldberg/Dex-Net-5.0/model_zoo/ryans_fcgqcnn_conversion.pt\")\n",
    "model = FCGQCNN()\n",
    "model.load_state_dict(w)\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "def preprocess_img(x, normal_crop=None):\n",
    "    # x = 255 - x # blender depth is inverted\n",
    "\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    x = cv2.dilate(x, kernel, iterations=1)\n",
    "    x = cv2.GaussianBlur(x, (5, 5), 5)\n",
    "    if normal_crop is None:\n",
    "        x = (x - x.mean()) / x.std()\n",
    "    elif normal_crop == \"normal\":\n",
    "        normalizers = (\n",
    "            0.59784445,\n",
    "            # 0.00770147890625,\n",
    "            .06873399\n",
    "        )  # mean, std value for depth images (from analyze.py script)\n",
    "        x = (x - normalizers[0]) / normalizers[1]\n",
    "    else:\n",
    "        x_crop = x[normal_crop[0]:normal_crop[1],normal_crop[2]:normal_crop[3]]\n",
    "        x = (x - x_crop.mean()) / x_crop.std()\n",
    "    # x *= 10\n",
    "    # x = cv2.resize(x, (60,60)) # 125,70\n",
    "    # pad = 15\n",
    "    # x = cv2.copyMakeBorder(x, pad, pad, pad, pad, cv2.BORDER_REPLICATE)\n",
    "    x = x.squeeze()\n",
    "    x = torch.tensor(x, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(\"cuda\")\n",
    "    return x\n",
    "    verlay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_names)\n",
    "print(cleaned_depth_ims.shape)\n",
    "# rgb_img_path = os.path.join(Path(file_names[3]).parent.parent, \"img\", f\"{Path(file_names[3]).stem}_l.jpg\")\n",
    "rgb_img_path = \"/home/apgoldberg/Dex-Net-5.0/gogs_test_data_real/raft_stereo1/img/0_l.jpg\"\n",
    "rgb_img = plt.imread(rgb_img_path)\n",
    "rgb_img = rgb_img[:,:]\n",
    "print(rgb_img.shape)\n",
    "plt.imshow(rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_rgb_grasp(rgb_img, grasp_out):\n",
    "    cmap = matplotlib.colormaps['rainbow']\n",
    "    target_height, target_width= rgb_img.shape[0], rgb_img.shape[1]\n",
    "\n",
    "    grasp_out_resized = (cv2.resize(\n",
    "        (grasp_out.squeeze()),\n",
    "        (target_width, target_height)\n",
    "    ))\n",
    "\n",
    "    color_mapped_grasps = cmap(grasp_out_resized)[:,:,:3]\n",
    "    to_show_alpha = (grasp_out_resized.squeeze() < .2)[:,:,None]\n",
    "\n",
    "    to_show = (cv2.resize(color_mapped_grasps, (target_width, target_height))).astype(np.uint8)\n",
    "    to_show = .5*to_show_alpha*to_show + (rgb_img).astype(np.uint8)\n",
    "\n",
    "    to_show = to_show.astype(np.uint8)\n",
    "    return to_show\n",
    "\n",
    "def plot_squares_with_numbers(arr):\n",
    "    viridis = matplotlib.colormaps['viridis']\n",
    "    color_div, color_add = np.nanmax(arr) - np.nanmin(arr), np.nanmin(arr)\n",
    "    dim1, dim2 = arr.shape[0], arr.shape[1]\n",
    "    fig, ax = plt.subplots(figsize=(32, 32))\n",
    "    for i in range(dim1):\n",
    "        for j in range(dim2):\n",
    "            color = viridis((arr[i][j] - color_add) / color_div)\n",
    "            ax.add_patch(plt.Rectangle((j, dim1 - i - 1), 1, 1, color=color))\n",
    "            ax.text(j + 0.5, dim1 - i - 1 + 0.5, str(round(arr[i][j], 2)), ha='center', va='center')\n",
    "\n",
    "    ax.set_xlim(0, dim2)\n",
    "    ax.set_ylim(0, dim1)\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_input_im = cleaned_depth_ims[3] + .35\n",
    "# base_input_im = cleaned_depth_ims[3,:-50,200:-50] + .3\n",
    "# base_input_im = cleaned_depth_ims[3,200:500,1000:1250] + .3\n",
    "# base_input_im[base_input_im > .775] = .84\n",
    "# base_input_im = cv2.resize(base_input_im, (34,34))\n",
    "plt.imshow(base_input_im)\n",
    "plt.show()\n",
    "\n",
    "def inference(input_im):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_im)\n",
    "    print(torch.max(output))\n",
    "    to_show = output.cpu().squeeze().unsqueeze(-1).numpy()\n",
    "    # plt.imshow(output.squeeze().cpu(), vmin=0, vmax=1)\n",
    "    # plt.show()\n",
    "    return output, to_show\n",
    "\n",
    "scales = np.arange(.05,.4,.05)\n",
    "# scales = [1]\n",
    "fig, axes = plt.subplots(max(len(scales) // 4 + 1, 2), 4, figsize=(16, 12))\n",
    "for i, scale in enumerate(scales):\n",
    "    # Resize the input image\n",
    "    resized_img = cv2.resize(base_input_im, None, fx=scale, fy=scale)\n",
    "    \n",
    "    # Preprocess the resized image\n",
    "    input_im = preprocess_img(resized_img, normal_crop=\"normal\")\n",
    "    # plt.imshow(input_im.squeeze().cpu())\n",
    "    # plt.show()\n",
    " \n",
    "    # Pass the preprocessed image through the model\n",
    "    output, to_show = inference(input_im)\n",
    "    plt.show(to_show)\n",
    "    # pad = 15\n",
    "    # to_show = cv2.copyMakeBorder(to_show, pad, pad, pad, pad, cv2.BORDER_CONSTANT, value=0)\n",
    "    # # plt.imshow(to_show, vmin=0, vmax=1)\n",
    "    # # plt.show()\n",
    "    \n",
    "    # # Prepare the output for visualization\n",
    "    # # to_show_rgb = cv2.resize(rgb_img, None, fx=scale, fy=scale)\n",
    "    # cmap = matplotlib.colormaps['gray']\n",
    "    # to_show_rgb = cmap(resized_img.squeeze())[:,:,:3]\n",
    "    # to_show[to_show < .2] = 0\n",
    "    # # to_show = composite_rgb_grasp(to_show_rgb, to_show)\n",
    "\n",
    "    axes[i//4, i%4].imshow(to_show)\n",
    "    axes[i//4, i%4].set_title(f'Scale: {scale}')\n",
    "    axes[i//4, i%4].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plot_squares_with_numbers(base_input_im.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = .3\n",
    "z_offsets = np.arange(.1, .2, .01)\n",
    "\n",
    "fig, axes = plt.subplots(max(len(z_offsets) // 4 + 1, 2), 4, figsize=(16, 12))\n",
    "for i, z_offset in enumerate(z_offsets):\n",
    "    base_input_im = cleaned_depth_ims[3] + z_offset\n",
    "    resized_img = cv2.resize(base_input_im, None, fx=scale, fy=scale)\n",
    "    input_im = preprocess_img(resized_img, normal_crop=\"normal\")\n",
    "\n",
    "    # Pass the preprocessed image through the model\n",
    "    output, to_show = inference(input_im)\n",
    "    pad = 15\n",
    "    to_show = cv2.copyMakeBorder(to_show, pad, pad, pad, pad, cv2.BORDER_CONSTANT, value=0)\n",
    "    \n",
    "    # Prepare the output for visualization\n",
    "    # to_show_rgb = cv2.resize(rgb_img, None, fx=scale, fy=scale)\n",
    "    cmap = matplotlib.colormaps['gray']\n",
    "    to_show_rgb = cmap(resized_img.squeeze())[:,:,:3]\n",
    "    to_show[to_show < .2] = 0\n",
    "    to_show = composite_rgb_grasp(to_show_rgb, to_show)\n",
    "\n",
    "    axes[i//4, i%4].imshow(to_show)\n",
    "    axes[i//4, i%4].set_title(f'z_add: {z_offset}')\n",
    "    axes[i//4, i%4].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(input_im.cpu().squeeze().unsqueeze(-1).numpy(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newdex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
