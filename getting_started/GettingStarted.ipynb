{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running GQ-CNN inference on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dexnet.grasp_model import DexNetBase as GQCNN\n",
    "from dexnet.torch_dataset import Dex3Dataset as DexDataset\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to directory containing \"tensors\" folder\n",
    "dataset_path = Path(\"../dataset/dexnet_3/dexnet_09_13_17\")\n",
    "\n",
    "# path to model checkpoint\n",
    "model_path = \"../model_zoo/DexNetBaseSuction.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DexDataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cpu\":\n",
    "    print(\"WARNING: using cpu\")\n",
    "\n",
    "model = GQCNN()\n",
    "weights = torch.load(model_path)\n",
    "model.load_state_dict(weights)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "samples = np.random.choice(2000000, 256, replace=False)\n",
    "model_input = torch.stack([dataset[i][0] for i in samples])\n",
    "gt_data = torch.stack([dataset[i][1] for i in samples])\n",
    "\n",
    "print(model_input.shape)\n",
    "print(gt_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = model_input.to(device)\n",
    "outputs = model(model_input).cpu()\n",
    "\n",
    "# Let's see what scores our model predicted for the positive grasps!\n",
    "pos_idx = torch.where(gt_data > .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = model_input.cpu()\n",
    "pos_images = model_input[pos_idx]\n",
    "labels = gt_data[pos_idx]\n",
    "predictions = outputs[pos_idx]\n",
    "\n",
    "# Visualizing positive suction grasp images\n",
    "\n",
    "fig, axes = plt.subplots(5, 5, figsize=(10, 10))\n",
    "\n",
    "for i in range(np.min([25, len(pos_images)])):\n",
    "    ax = axes[i // 5, i % 5]  # Determine the subplot position\n",
    "    ax.imshow(pos_images[i].permute(1, 2, 0))\n",
    "    ax.axis('off')  # Turn off axis for cleaner display\n",
    "    ax.text(0.5, -0.1, f\"GT: {labels[i].item():.2f}, Pred: {predictions[i].item():.2f}\", \n",
    "            transform=ax.transAxes, ha=\"center\")  # Add label and prediction as text at the bottom\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running FC-GQ-CNN inference on other depth images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dexnet.grasp_model import DexNetBase as GQCNN\n",
    "from dexnet.torch_dataset import Dex3Dataset as DexDataset\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from dexnet.grasp_model import BaseFCGQCNN as FCGQCNN\n",
    "from dexnet.grasp_model import HighResFCGQCNN as HighResFCGQCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../model_zoo/SuctionFCGQCNN.pth\"\n",
    "# model_path = \"../model_zoo/ParallelJawFCGQCNN.pth\"\n",
    "depth_im_path = \"example_depth.png\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cpu\":\n",
    "    print(\"WARNING: using cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCGQCNN()\n",
    "weights = torch.load(model_path)\n",
    "model.load_state_dict(weights)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Image pre-processing functions\"\"\"\n",
    "\n",
    "def blur(img):\n",
    "    kernel = np.ones((15,15), np.uint8)\n",
    "    img = cv2.dilate(img, kernel, iterations=1)\n",
    "    img = cv2.GaussianBlur(img, (15, 15), 5)\n",
    "\n",
    "    return img\n",
    "\n",
    "def processImg(img):\n",
    "    \"\"\"\n",
    "    example preprocessing for numpy depth image shape (width, height, 1) before inference\n",
    "    blurs, normalizes, resizes, pads, and converts to tensor\n",
    "    NOT batched operation.\n",
    "    \"\"\"\n",
    "    img = img.mean(axis=-1)\n",
    "\n",
    "    img = blur(img)\n",
    "\n",
    "    img = (img - img.mean()) / img.std()\n",
    "    img = cv2.resize(img, (40, 40))\n",
    "\n",
    "    pad = 15\n",
    "    img = cv2.copyMakeBorder(img, pad, pad, pad, pad, cv2.BORDER_REPLICATE)\n",
    "\n",
    "    img = torch.tensor(img, dtype=torch.float32).squeeze().unsqueeze(0)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_im = cv2.imread(depth_im_path)\n",
    "# this image happens to be inverted\n",
    "depth_im = 255 - depth_im\n",
    "depth_im = np.rot90(depth_im)\n",
    "depth_im_to_display = depth_im\n",
    "\n",
    "assert len(depth_im.shape) == 3 and depth_im.shape[2] == 3, f\"image expected to be (H, W, 3) for this demo, image is shape: {depth_im.shape}\"\n",
    "\"\"\"\n",
    "Note that this processNumpy function is not batched \n",
    "and should be adjusted based on your image distrbution\n",
    "\"\"\"\n",
    "depth_im = processImg(depth_im)\n",
    "\n",
    "\"\"\"\n",
    "We need to lower the resolution of our image so that the 32x32 crops still contain large portions of the object.\n",
    "This keeps the images in distribution\n",
    "We also add padding so we'll make predictions at the edge of the original image\n",
    "\"\"\"\n",
    "plt.imshow(depth_im.squeeze().unsqueeze(-1))\n",
    "\n",
    "# The image we feed into our model should be shape (B, 1, H, W)\n",
    "depth_im = depth_im.unsqueeze(0)\n",
    "\n",
    "\n",
    "print(depth_im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_im = depth_im.to(device)\n",
    "output_heatmap = model(depth_im).cpu()\n",
    "output_heatmap = output_heatmap.squeeze().unsqueeze(-1).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_heatmap = output_heatmap\n",
    "filtered_heatmap[output_heatmap <= .2] = 0\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(filtered_heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_im = depth_im.to(device)\n",
    "\n",
    "high_res_model = HighResFCGQCNN() \n",
    "high_res_model.load_state_dict(torch.load(model_path))\n",
    "high_res_model = high_res_model.to(device)\n",
    "\n",
    "output_heatmap = high_res_model(depth_im).cpu()\n",
    "output_heatmap = output_heatmap.squeeze().unsqueeze(-1).detach().numpy()\n",
    "print(output_heatmap.shape)\n",
    "plt.imshow(output_heatmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_heatmap = output_heatmap\n",
    "filtered_heatmap[output_heatmap <= .2] = 0\n",
    "depth_im = depth_im.cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 8))\n",
    "axes[0].imshow(filtered_heatmap)\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(depth_im_to_display)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
