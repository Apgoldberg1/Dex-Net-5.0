# parallel jaw config

name: "DexNet2"
model: "dexnet3"
dataset: "dexnet2"
optimizer:
  name: "SGD"
  learning_rate: 0.005
  momentum: .9
  scheduler_gamma: .90   #original is .95, but every .66 epochs. This is per epoch
  weight_decay: .0005
training:
  dataset_path: "dataset/dexnet_2/dexnet_2_tensor"
  batch_size: 256
  num_epochs: 25
  resize: false
  GT_threshold: 0.002
  wandb: true
  ordered_split: true
  pos_weight: 1 # make 1 for no oversampling
outputs:
  save_directory: "outputs/DexNet2"
  save_name: "dex2_first"
  training_print_every: 500
  val_print_every: 2000
  save_every_x_epoch: 10

  

